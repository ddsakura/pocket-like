import os

os.environ["TOKENIZERS_PARALLELISM"] = "false"
#!/usr/bin/env python3
"""
CLI å·¥å…·ï¼Œç”¨æ–¼æ“ä½œå’Œç®¡ç†æ›¸ç±¤è³‡æ–™åº«
"""

import json
import subprocess
import time

import chromadb
import click
import requests
import sqlite_utils

# åˆå§‹åŒ– Chroma å®¢æˆ¶ç«¯ä¸¦é€£æ¥è‡³æ›¸ç±¤é›†åˆ
chroma_client = chromadb.PersistentClient(path="./chroma")
collection = chroma_client.get_or_create_collection("bookmarks")


@click.group()
def cli():
    """æ›¸ç±¤ç®¡ç†ç³»çµ±å‘½ä»¤è¡Œå·¥å…·"""
    pass


@cli.command()
@click.argument("query")
@click.option("--tags", help="éæ¿¾æŒ‡å®šæ¨™ç±¤ï¼ˆç”¨é€—è™Ÿåˆ†éš”ï¼‰", default=None)
@click.option("--limit", help="æœ€å¤šå›å‚³å¹¾ç­†çµæœ", default=5)
def search(query, tags, limit):
    """ä»¥é—œéµå­—å’Œæ¨™ç±¤æœå°‹æ›¸ç±¤"""
    result = collection.query(query_texts=[query], n_results=limit)
    for doc, meta in zip(result["documents"][0], result["metadatas"][0]):
        if tags:
            tag_list = tags.split(",")
            if not any(tag in meta.get("tags", "") for tag in tag_list):
                continue
        print(f"- {meta['title']} ({meta['url']})")


@cli.command(name="dump-all")
def dump_all():
    """åˆ—å‡ºæ‰€æœ‰æ›¸ç±¤"""
    all_results = collection.get()
    for doc, meta in zip(all_results["documents"], all_results["metadatas"]):
        tags = ", ".join(meta.get("tags", [])) if isinstance(
            meta.get("tags"), list) else meta.get("tags", "")
        print(f"- {meta['title']} ({meta['url']}) [tags: {tags}]")


@cli.command(name="excerpt-content")
@click.argument("url")
def excerpt_content(url):
    """å¾ Readability API æ“·å–ä¸»æ–‡å…§å®¹"""
    try:
        response = requests.get(f"http://localhost:3000/readability?url={url}")
        response.raise_for_status()
        data = response.json()
        print(
            f"\nâœ… æ“·å–æˆåŠŸ:\n\nTitle: {data.get('title')}\nContent:\n{data.get('content')[:500]}...\n"
        )
    except Exception as e:
        print(f"âŒ æ“·å–å¤±æ•—: {e}")


@cli.command(name="backfill-excerpts")
def backfill_excerpts():
    """ç‚ºå°šæœªæ“·å–å…§å®¹çš„æ›¸ç±¤è£œä¸Š extract ä¸¦æ›´æ–° chromadb"""
    try:
        script_path = os.path.abspath(
            os.path.join(os.path.dirname(__file__), "..", "readability",
                         "readability-server.js"))
        process = subprocess.Popen(["node", script_path])
        print("ğŸš€ å·²å•Ÿå‹• readability-server.jsï¼Œç¨å€™å•Ÿå‹•å®Œæˆâ€¦")
        time.sleep(2)

        db = sqlite_utils.Database("bookmarks.db")
        table = db["bookmarks"]

        rows = list(table.rows_where("excerpt IS NULL OR excerpt = ''"))
        print(f"ğŸ” ç™¼ç¾ {len(rows)} ç­†å¾…è£œå…§å®¹çš„æ›¸ç±¤")

        try:
            for row in rows:
                url = row["url"]
                id = row["id"]
                try:
                    response = requests.get(
                        f"http://localhost:3000/readability?url={url}")
                    response.raise_for_status()
                    data = response.json()
                    content = data.get("excerpt", "").strip()

                    if not content:
                        print(f"âš ï¸  {url} æ“·å–çµæœç‚ºç©ºï¼Œç•¥é")
                        continue

                    # æ›´æ–° SQLite
                    table.update(id, {"excerpt": content})

                    # æ›´æ–° ChromaDB
                    collection.upsert(documents=[content],
                                      ids=[id],
                                      metadatas=[{
                                          "url": url,
                                          "title": row["title"],
                                          "tags": row.get("tags", [])
                                      }])
                    print(f"âœ… å·²æ›´æ–°: {url}")
                except Exception as e:
                    print(f"âŒ æ“·å–å¤±æ•— {url}: {e}")
        finally:
            process.terminate()
            print("ğŸ›‘ å·²é—œé–‰ readability-server.js")
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•å•Ÿå‹• readability server: {e}")


def generate_tags_with_ollama(title, excerpt):
    """ä½¿ç”¨ Ollama æ¨¡å‹ç”Ÿæˆæ¨™ç±¤ï¼Œè¿”å›æ¨™ç±¤åˆ—è¡¨"""
    import json
    import re
    import socket
    import subprocess
    import time

    def is_ollama_running():
        try:
            with socket.create_connection(("localhost", 11434), timeout=1):
                return True
        except OSError:
            return False

    if not is_ollama_running():
        print("Ollama not running, attempting to start it...")
        subprocess.Popen(["ollama", "serve"])
        time.sleep(3)  # wait a moment to let it start

    prompt = f"""
è«‹æ ¹æ“šä¸‹åˆ—æ¨™é¡Œèˆ‡æ‘˜è¦å»ºè­° 1 åˆ° 3 å€‹æ¨™ç±¤ã€‚è«‹åƒ…å›å‚³ä¸€çµ„çµæœï¼Œæ ¼å¼ç‚º JSON é™£åˆ—ï¼ˆä¾‹å¦‚ ["AI", "æ•™è‚²"]ï¼‰ï¼Œä¸è¦æä¾›å…¶ä»–èªªæ˜ã€æ–‡å­—æˆ–å¤šçµ„ç¯„ä¾‹ã€‚

Title: {title}
Excerpt: {excerpt}
"""

    try:
        result = subprocess.run(["ollama", "run", "gemma3:latest"],
                                input=prompt.encode("utf-8"),
                                capture_output=True,
                                check=True)
        output = result.stdout.decode("utf-8").strip()
        match = re.search(r"\[.*?\]", output, re.DOTALL)
        if match:
            tags_json = match.group(0)
            tags = json.loads(tags_json)
            return tags
        else:
            print(f"âš ï¸ ç„¡æ³•è§£ææ¨™ç±¤ JSON é™£åˆ—ï¼ŒåŸå§‹è¼¸å‡º: {output}")
            return []
    except Exception as e:
        print(f"âŒ æ“·å–æ¨™ç±¤å¤±æ•—: {e}")
        return []


@cli.command(name="backfill-tags")
def backfill_tags():
    """ç‚ºå°šæœªç”Ÿæˆæ¨™ç±¤çš„æ›¸ç±¤è£œä¸Š tag ä¸¦æ›´æ–° chromadb"""
    db = sqlite_utils.Database("bookmarks.db")
    table = db["bookmarks"]
    rows = list(
        table.rows_where(
            "((tags IS NULL OR tags = '' OR tags = '[]') AND (excerpt IS NOT NULL AND excerpt != ''))"
        ))

    print(f"ğŸ” ç™¼ç¾ {len(rows)} ç­†å¾…è£œæ¨™ç±¤çš„æ›¸ç±¤")

    for row in rows:
        tags = generate_tags_with_ollama(row['title'], row['excerpt'])
        if tags:
            # æ›´æ–° SQLite
            table.update(row["id"], {"tags": tags})

            # æ›´æ–° Chroma
            collection.upsert(documents=[row["excerpt"]],
                              ids=[row["id"]],
                              metadatas=[{
                                  "url": row["url"],
                                  "title": row["title"],
                                  "tags": ", ".join(tags)
                              }])
            print(f"âœ… å·²ç‚º {row['url']} å»ºç«‹æ¨™ç±¤: {tags}")
        else:
            print(f"âš ï¸ ç„¡æ³•ç‚º {row['url']} ç”Ÿæˆæ¨™ç±¤ï¼Œç•¥é")


@cli.command(name="suggest-tags")
@click.argument("title")
@click.argument("excerpt")
def suggest_tags(title, excerpt):
    """ä½¿ç”¨ Ollama çš„ mixtral æ¨¡å‹å»ºè­°æ¨™ç±¤"""
    tags = generate_tags_with_ollama(title, excerpt)
    if tags:
        print("ğŸ“Œ å»ºè­°æ¨™ç±¤:")
        print(json.dumps(tags, ensure_ascii=False))
    else:
        print("âš ï¸ ç„¡æ³•ç”¢ç”Ÿæœ‰æ•ˆæ¨™ç±¤ã€‚")


if __name__ == "__main__":
    # åŸ·è¡Œå‘½ä»¤åˆ—ä»‹é¢
    cli()
